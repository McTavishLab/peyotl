\documentclass{bioinfo}
\copyrightyear{2014}
\pubyear{2014}
\usepackage{xspace}
\newcommand{\ps}{phylesystem\xspace}
\newcommand{\otol}{Open Tree of Life\xspace}
\newcommand{\mthcomment}[1]{{\color{red} \textsc{#1}}\xspace}

\usepackage{natbib}
\bibliographystyle{apalike}

\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=red, citecolor=black, hyperindex=true, backref}
\begin{document}
\firstpage{1}
\title[phylesystem git phylostore]{Phylesystem: a git-based data store for community curated phylogenetic estimates}

\author[McTavish\textit{et~al}]{
    Emily Jane McTavish,$^{1,2}$
    Mark T.~Holder,$^{1,2}$\footnote{to whom correspondence should be addressed}~
    Ann Other Author,$^{3}$
    Ann Other Author,$^{4}$
    Ann Other Author,$^{5}$ $\ldots$
}
\address{$^{1}$Department of Ecology and Evolutionary Biology, University of Kansas, Lawrence KS, USA\\
$^{2}$Heidelberg Institute of Theoretical Studies, Heidelberg, Germany}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}
\section{Motivation:}
Archival storage of phylogenetic estimates from published studies can be accomplished
    using general platforms, like Dryad, or TreeBase.
Those services fulfill a crucial role in helping phylogenetic studies be more transparent
    and reproducible.
However, available trees require some editing to improve the accuracy and reusability of the phylogenetic statements.
Many trees in data archives or supplemental files from publications are incorrectly rooted.
Establishing the linkage between tip labels used in a tree and taxa in a single taxonomy 
    dramatically improves the ability of other researchers to reuse phylogenetic estimates.
Because process of curating a published phylogenetic estimate is not error-free,
    retaining a full record of the provenance edits to a tree is crucial for
    openness, allowing editors to receive credit for their work, and
    making errors introduced during curation easier to correct.

\section{Results:}
Here we report the development of software infrastructure to support the open curation of 
    phylogenetic data by the community of biologists.
The backend of the system for provides an interfaces for the standard database operations of
    creating, reading, updating, and deleting records by creating commits in a git repository.
The record of the history of edits to a tree is preserved by git's version control features.
Hosting this datastore on GitHub provides a open access to the data store using tools 
    familiar to many developers.
The server running the ``phylesystem-api'' to wrap the interactions with git, and GitHub has been 
    deployed. 
The \otol project has also developed and deployed a JavaScript application that uses the \ps-api and
    other web services to enable curation of published phylogenetic statements
\section{Availability:}
Source code for the web-service layer is available at \url{https://github.com/OpenTreeOfLife/phylesystem-api}.\\
The library \url{https://github.com/OpenTreeOfLife/peyotl} is a prerequisite.
The data store can be cloned from \\
\url{https://github.com/OpenTreeOfLife/phylesystem}
\section{Contact:} \href{mailto:mtholder@gmail.com}{mtholder@gmail.com}

\end{abstract}

\section{Introduction}

Characterizing and systematizing relationships among species has been a goal of biologists since Linnaeus \cite{Linne1758}.
The phylogenetic systematics ``revolution'' of the 1960's focused most of the effort toward this goal around the 
    task of estimating phylogenetic relationships as the most important tool for classifying life on Earth.
The appreciation of the importance of phylogenetically-aware comparative methods \citep[e.g.][]{Felsenstein1985Comp} and 
the rapid growth in availability of molecular data have led to dramatic increases in the number phylogenetic estimates.


Unfortunately, capturing the inputs and outputs of a phylogenetic analysis in a rich, uniform manner is
    difficult and error prone.
Some archiving servies, such as Dryad \citep{Dryad}, have reacted to this challenge by accepting a 
    wide range of inputs and making few or no
    guarantees to users of the database that the records will be in any particular form.
This encourages sharing of data by making the submission process fast and easy.
However, if the authors submitting the data are not conscientious in their explanations of the data, it can be difficult
    for users of the data to reliably extract all of the information in the archive.

On the opposite end of the spectrum, some databases require data providers to conform to more rigorous
    standards with respect to input format and content.
Since 1994, TreeBase \citep{SandersonDPE1994} and its successor TreeBase version 2 have served
    as the primary archival data stores for phylogenetic estimates and the data that these trees are based on.
As of 2014, TreeBase contains 4076 studes and over 12,800 trees \citep{TreeBaseWebCite}.
Many journals encourage or require that each publications that estimate phylogenetic trees create a TreeBase
    deposition for the publication.
TreeBase produces a much more feature-rich interface for phylogenetic queries than DataDryad, because each 
    TreeBase submission is parsed thoroughly and converted to a set of records in a relational database.
The downside of this approach is that it requires that submissions correspond to a more uniform format.

If there were a universally used, rich file format for phylogenetics analyses, then the restriction would not be a
    serious hurdle.
Unfortunately, file formats read and written by tree estimation tools are idiosyncratic and often so 
    terse that they omit useful information about the analysis.
See \citet{StoltzfusEtAl2012} for a discussion of this topic and others challenges relating to the archiving of phylogenetic estimates.
Thus the TreeBase submission process usually requires the authors of a data package to reformat their data, correct
    the rooting of the tree, alter the labels of the tips of the tree, {\em etc}.
    The TreeBase project provides a 12-minute video explaining
    \footnote{\url{http://treebase.org/treebase-web/submitTutorial.html}} 
    the non-trivial procedure of preparing a submission.
Because these steps are often taken after the analyses for a publication are complete, it is more likely 
    errors introduced in the submission process will not be corrected until an observant user tries to reuse the data.

The state of phyloinformatic archiving described above suggests the need for a datastore that allows the 
    community of biologists to improve the accuracy and re-usability of published phylogenetic statements.
We describe here an attempt at one such system, which we call ``phylesystem'' because it uses a set of
    versioned text files on the filesystem (and the fact that we {\em love} bad puns).
The goal of \ps is not to replace systems like Dryad or TreeBase, but to complement them by 
    providing phylogenetic statements in consistent format while retaining the history of
    edits that were made to the raw data.
We anticipate that most of the trees stored in \ps will be associated with permanent, static archives
    elsewhere (usually either TreeBase or Dryad).

\subsection{Approach}
One of the motivations of the design of \ps was to support the \otol's need for a curated set
    of rooted trees that have been aligned to a common taxonomy.
However, one of the goals of the \otol project was to support infrastructure for phylogenetic
    research in an open way that encourages the community of biologists to participate in the 
    collection and curation of our knowledge of the phylogenetic relationships of life on Earth.

Thus, our goals when designing \ps were to build: 
\begin{enumerate}
    \item  a data store capable of:
        \begin{enumerate}
            \item \label{fewKStudies} storing capabale of storingfrom thousands of published studies. We do not intend to store the 
                data upon which the phylogenetic estimates are based.
            \item \label{fewTreesPerStudy} storing a few exemplar trees from each study. We do not anticipate using the system will be used 
                to store thousands of trees from 
                a study (such as each sampled tree from a bootstrapping or MCMC analysis, for example).
            \item handling rich annotation of the trees, the taxa to which they refer to, and metadata describing 
                the analysis that produced the trees.
            \item storing the full history of changes made to a study and its trees, 
                including an identifier to indicate who made the changes.
        \end{enumerate}
    \item web services around the data store to support a user-friendly curation application run in the user's browswer.
    \item a loosely coupled system that would allow the community of biologists in interact with the data in a wide variety of ways.
\end{enumerate}
From these requirements, we chose to implement \ps as a set of software wrappers around a data store
    which consists of a git \citep{git} repository of phylogenetic statements serialized as a JavaScript Object Notation (JSON).
The data model used for the JSON is a close derivative of the NeXML standard for data interoperability\citep{NeXML}.

The combination of a relatively small amount of data that we expected to store (points \ref{fewKStudies} and \ref{fewTreesPerStudy} above)
    and the small number of edits anticipated for each study imply that
    raw performance of the basic CRUD operations was unlikely to be a bottleneck for most
    uses of the \ps.
This made it feasible to use text files as form of data store.
Indeed the total size of the stored data (in JSON format with a line ending after 
    every field) is only around 150MB.
    Thus far, in the six months since deployment of the study curation interface, the mean number of edits 
    per study is only \mthcomment{Need to calculate this from git history - it is a small, number though}.

The requirement \# 3 of maintaining a history of changes (including an identifier for the editor) fits
naturally with software version control systems.
We note that wiki engines such as Gollum use git version control as a backend database.
See also \url{https://speakerdeck.com/bkeepers/git-the-nosql-database}

The requirement (\#4 above) to support rich annotations, suggested the NeXML data standard which was designed
    to allow arbitrarily complex annotations of the entities that are crucial to phylogenetic statements.
Because \ps does not store character data, these entities are the operational taxonomic units (OTUs), trees, nodes, and edges.

We opted to version JSON files to make it more efficient for the server to provide data for a client-side
curation application that is written in JavaScript.
We developed some syntactic conventions for converting XML to JSON tersely (described below as NexSON v1.2 and v1.0).
The adoption of these conventions reduced the web-service data payload size by appoximately 50\%, and made it more
    feasible to load each study into the memory of the client's browser.
Despite the departure from the NeXML syntax, the \ps maintains the ability to export and ingest NeXML files.
Fundamentally the data model of the system is essentially that of NeXML.

The final requirement of producing a maximally open, loosely coupled system suggested the use of git
    as the version control system of choice.
git is the most popular distributed version control system.
Distributed version control systems do not require a single, ``primary'' repository; each 
    copy of the repository maintains a copy of the entire history, and sharing documents between
    repositories is a peer-to-peer interaction.
Most users will probably think of the copy of the \ps repositories on the servers as GitHub 
    as the canonical version of the data store because that clone of the repository is the most
    easily visible and browsable copy.
However other users of git are free to fork the repository and maintain variants of the data store, while
    still maintaining the ability to pull in changes from biologists who contribute edits to the 
    version of the repository on GitHub.
The \otol project placed all of the code for the \ps under permissive, BSD-style licenses and the
    project makes no claim of ownership to any of the data in the repository.
In most database-driven web services, the group of core maintainers who have shell access and 
    database passwords are the only people who have full access to the data in a data store.
Posting frequent dumps of a database can allow other users to obtain local copies of the data 
    for expensive, {\em ad hoc} calculations.
However, a local version of the data is clearly a secondary instance and the maintainers of the 
    canonical version of the database have {\em de facto} ownership of the project.
We hope that adopting a truly distributed backend for the datastore will reinforce the goals
    of the \otol project to build tools that the community of biologists can use and control
    (rather than claiming ownership on the data store in perpetuity).

Currently the \ps web services use GitHub authentication for ``curators'' who edit studies.
The requirement that editors authenticate allows the provenance of each edit to include 
    a user name for the curator.
However, the \otol project does not need to maintain a database of users and passwords (or hashes).
Thus, their is no private data base that would be required for someone to fork and maintain their
    own version of the corpus (on GitHub or elsewhere).

\begin{methods}
\section{Methods}
The basic workflow of a curation session is:
\begin{enumerate}
    \item the curator loads the page in his/her browser, which fetches the javascript application into the web browser.
    \item a list of studies in the system is loaded from information in a study indexing service (called ``oti'', see below).
    \item the user selects the study they would like to view.
    \item the javascript app requests the study in NexSON form from the \ps server. The javascript app also receives the git commit SHA of the 
        repository at this point.
    \item the user may browse the studies and download representations of the data in NeXML, NexSON, NEXUS, or newick formats without logging in.
    \item if the user wants to edit a study, he/she must have a GitHub account (which are available free of charge) and must authenticate at this point.
    \item the curator may correct tip taxon assignments using taxonomic name resolution services (the ``taxomachine'' web services, which are described
        elsewhere) or fix the rooting information about the tree. The data model of the study is modified in the memory space of the 
        browser and maintained there until the curator chooses to save the study.
    \item to save, the javascript app uses and http PUT verb and includes the data (in NexSON) and the SHA.
    \item the \ps-api code validates the NexSON (rejecting the request if the data is not a legal NexSON document).
    \item if the data is legal, then a new git commit is created with the SHA provided in the PUT request as the parent commit. The commit is place on a ``work-in-progress'' branch in the git history to assure that the data is stored with no chance of conflict.
    \item if the study in question has not changed in the master branch since the parent SHA, then the edit can safely be merged to the master branch.
        If this is the case, the merge is done and the work-in-progress branch is deleted.
        If the version of the study on the master has changed (e.g. if 2 users are simultaneously editing the same study),
        then the merge is not done. The data will be saved on the server, but not merged into the master branch.
    \item If the new edit was successfully merged, then an event is triggered to tell the server to push the new master branch changes to the GitHub version
        of the repository.
    \item A response is returned to the javascript code indicating whether or not the data was 
        merged to the master branch and including the new SHA that will serve as the parent for future commits.
    \item If a merge to the master branch, the updated master branch will be pushed to GitHub.
    \item Using GitHub webhooks as a callback mechanism, the push to generates a POST http communication to trigger reindexing
        of the affected study by the oti tool.
\end{enumerate}

\section{Implementation}
Currently the \ps is implemented in a python application using web2py for as the web framework.
Most of the functionality for validating the data and creating the git commits is accomplished using
    parts of the peyotl library (manuscript in preparation).

In the current implementation, the creation of a new commit and merge are done with git's ``checkout'', ``add'', and ``commit'' commands.
This means that the repository must be locked for the duration of these events 
    to ensure that a single thread completes this set of operations.
If the system encounters heavier use, then lower level git commands 
    are available that allow one to construct a commit without checking out
    the parent commit's version of all files.
\end{methods}

\section{Discussion}

\section{Conclusion}

\section*{Acknowledgement}

\paragraph{Funding\textcolon}

\bibliography{phylesystem}

\section{Useful text that we might want to reinstate}
However, making phylogenies re-usable by researchers not involved in the original analyses often requires fairly extensive curation.
The tip labels on trees often have meaning in the context of that analysis that are not easily translated across studies. 
For example, the tip labels may contain abbreviations, or lab sample codes.
Data reuse requires standardizing labels so that they are meaningful across studies.
Taxonomic name recognition services, such as ?? Plant something, or ?? taxomachine, can attempt to automatically map names to
operational taxonomic units, but this process also requires come subjective human intervention. 

By supporting contribution and curation in an open and accessible manner, we hope to provide a database with longterm utility and widespread adoption.

          - many curators!, possibly editing studies at the same time!
          - long term used, ideally.

           Implementation of database (structure):
      - FIGURE TO REPRESENT!
      - File Formats
         - Explain Nexson... UGH
         
      - Sharded git repository
          Describe full structure 
          
      - External library to index studies (is OTI described somewhere?)
      - peyotl...
      
      Curation via app:
        - main repo on server -> mirror -> github
            ??? do pulls from github happen? wholes cycle *can* be reversed.
        - dealing with WIP branches.
        *Even conflicts that Git may find mergable"*
        return merge conflicts to user

 Discussion:
   Generalizability:
      Git may be a good option for any non-static data store.
      
  Future directions

  
\bibliography{phylesystem}  



\section{notes}
  --------------------------------------------------------------------
Key Points:
Pros
 - Versioned
 - Familiar to users
 - Comes with a lot of great machinery already enabled

diffing doesn't always behave well.
simple data structures behave oddly...

Meaningful merges
 -- immutable 
 -- can't

Is this model applicable for database store

General model based decisions:
- Decisions that studies won't move.

- given the study ID what shard will it be in.

- scales better in general.

- Data is so open.



 The current (as of May) data set
 Community contributed phylogenies

 - 6745 trees from 2914 published studies
 - 1188 trees from 991 studies partly curated 
 - 335 trees from 327 studies completely curated and included in the synthetic tree.

 The problem:
 - Large data set: Thousands of phylogenies, and always growing (hopefully!)
 - Each phylogeny requires some hand curation, often by multiple people
 - Need to be readily accessible, and editable by interested researchers

Curation

 Potential data store options:
 - SQL database
 - Mongo, couchDB
 - git/github

 Curation
- Work in progress branch is created upon curation  
- If study hasn't been edited by someone else, changes are automatically merged.  
- Otherwise, merged changes are returned to curator to accept or reject
- Updates pushed to GitHub after each commit
 Features
 - Tracking curation attribution  
- Some subjective choices, edits made by many in the community over time

 Curation

 Features
 These trees are the backend for OpenTree showpiece  
 the synthetic tree!
 - but also a useful datastore for other researchers
 - Repo is hosted on GitHub, entire data store can be easily cloned and updated
 - Anyone can easily download all the data!
 
 Features
- Hosting on Github
- Free  
- Familar to  many in the field


 Potential issues:
 - Phylogenies are hard to diff - e.g. rerooting changes everything!
 - Nexson are not a line based format
 - Repo size limits on github


 In the future:

 - Semantic diffs  
 - Pull requests
 
  
Generality:
Is a git-based datastore right for your project?

 - Maybe! Any of the OpenTree software team are happy to chat about pros and cons.

NSF AVATOL \#1208809  


\end{document}
